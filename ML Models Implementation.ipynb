{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae062d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import log, exp\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62e78483",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datasets & Utilities\n",
    "\n",
    "def make_regression_data(n=200, d=1, noise=1.0, random_state=0):\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    X = rng.randn(n, d)\n",
    "    w = rng.randn(d,)\n",
    "    y = X.dot(w) + noise * rng.randn(n)\n",
    "    return X, y, w\n",
    "\n",
    "def make_classification_data(n=200, d=2, random_state=0):\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    X0 = rng.randn(n//2, d) + np.array([-2.0]*d)\n",
    "    X1 = rng.randn(n//2, d) + np.array([2.0]*d)\n",
    "    X = np.vstack([X0, X1])\n",
    "    y = np.array([0]*(n//2) + [1]*(n//2))\n",
    "    perm = rng.permutation(n)\n",
    "    return X[perm], y[perm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d762d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Supervised- Linear Regression\n",
    "\n",
    "class LinearRegressionClosedForm:\n",
    "    \"\"\"\n",
    "    Ordinary Least Squares (OLS) linear regression.\n",
    "    Theory: Solve for w in closed-form: w = (X^T X + lambda I)^{-1} X^T y\n",
    "    This implementation supports an L2 regularization parameter `lam`.\n",
    "    \"\"\"\n",
    "    def __init__(self, fit_intercept=True, lam=0.0):\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.lam = lam\n",
    "        self.coef_ = None\n",
    "        self.intercept_ = 0.0\n",
    "\n",
    "    def _add_intercept(self, X):\n",
    "        if self.fit_intercept:\n",
    "            return np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "        return X\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X0 = self._add_intercept(X)\n",
    "        n, p = X0.shape\n",
    "        A = X0.T.dot(X0)\n",
    "        A += self.lam * np.eye(p)\n",
    "        w = np.linalg.solve(A, X0.T.dot(y))\n",
    "        if self.fit_intercept:\n",
    "            self.intercept_ = w[0]\n",
    "            self.coef_ = w[1:]\n",
    "        else:\n",
    "            self.intercept_ = 0.0\n",
    "            self.coef_ = w\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return X.dot(self.coef_) + self.intercept_\n",
    "\n",
    "class LinearRegressionGD:\n",
    "    \"\"\"\n",
    "    Linear regression using gradient descent (MSE loss).\n",
    "    \"\"\"\n",
    "    def __init__(self, lr=1e-2, n_iters=1000, fit_intercept=True, lam=0.0):\n",
    "        self.lr = lr\n",
    "        self.n_iters = n_iters\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.lam = lam\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X0 = np.hstack([np.ones((X.shape[0], 1)), X]) if self.fit_intercept else X\n",
    "        n, p = X0.shape\n",
    "        w = np.zeros(p)\n",
    "        for i in range(self.n_iters):\n",
    "            preds = X0.dot(w)\n",
    "            grad = (2.0/n) * X0.T.dot(preds - y) + 2*self.lam*w\n",
    "            w -= self.lr * grad\n",
    "        if self.fit_intercept:\n",
    "            self.intercept_ = w[0]\n",
    "            self.coef_ = w[1:]\n",
    "        else:\n",
    "            self.intercept_ = 0.0\n",
    "            self.coef_ = w\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return X.dot(self.coef_) + self.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7064378f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Supervised: Logistic Regression\n",
    "\n",
    "class LogisticRegressionGD:\n",
    "    \"\"\"\n",
    "    Binary logistic regression trained with batch gradient descent.\n",
    "    Uses sigmoid activation and cross-entropy loss.\n",
    "    Supports L2 regularization.\n",
    "    \"\"\"\n",
    "    def __init__(self, lr=1e-2, n_iters=1000, fit_intercept=True, lam=0.0):\n",
    "        self.lr = lr\n",
    "        self.n_iters = n_iters\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.lam = lam\n",
    "\n",
    "    def _sigmoid(self, z):\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X0 = np.hstack([np.ones((X.shape[0], 1)), X]) if self.fit_intercept else X\n",
    "        n, p = X0.shape\n",
    "        w = np.zeros(p)\n",
    "        for i in range(self.n_iters):\n",
    "            z = X0.dot(w)\n",
    "            preds = self._sigmoid(z)\n",
    "            grad = (1.0/n) * X0.T.dot(preds - y) + 2*self.lam*w\n",
    "            w -= self.lr * grad\n",
    "        if self.fit_intercept:\n",
    "            self.intercept_ = w[0]\n",
    "            self.coef_ = w[1:]\n",
    "        else:\n",
    "            self.intercept_ = 0.0\n",
    "            self.coef_ = w\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        z = X.dot(self.coef_) + self.intercept_\n",
    "        p = self._sigmoid(z)\n",
    "        return np.vstack([1-p, p]).T\n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        return (self.predict_proba(X)[:,1] >= threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c753fbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Supervised: k-Nearest Neighbor\n",
    "\n",
    "class KNNClassifier:\n",
    "    \"\"\"\n",
    "    Simple k-Nearest Neighbors classifier using brute-force distance.\n",
    "    \"\"\"\n",
    "    def __init__(self, k=5):\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        preds = []\n",
    "        for x in X:\n",
    "            dists = np.linalg.norm(self.X_train - x, axis=1)\n",
    "            idx = np.argsort(dists)[:self.k]\n",
    "            vals = self.y_train[idx]\n",
    "            preds.append(Counter(vals).most_common(1)[0][0])\n",
    "        return np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df6b0ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supervised: Decision Tree (classification)\n",
    "\n",
    "class DecisionTreeClassifierSimple:\n",
    "    \"\"\"\n",
    "    A compact recursive CART-like decision tree for classification.\n",
    "    Uses Gini impurity and axis-aligned splits.\n",
    "    Not optimized for performance; educational implementation.\n",
    "    \"\"\"\n",
    "\n",
    "    class Node:\n",
    "        def __init__(self, gini, n_samples, predicted_class):\n",
    "            self.gini = gini\n",
    "            self.n_samples = n_samples\n",
    "            self.predicted_class = predicted_class\n",
    "            self.feature_index = None\n",
    "            self.threshold = None\n",
    "            self.left = None\n",
    "            self.right = None\n",
    "\n",
    "    def __init__(self, max_depth=5, min_samples_split=2):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.n_classes_ = None\n",
    "        self.n_features_ = None\n",
    "        self.root = None\n",
    "\n",
    "    def _gini(self, y):\n",
    "        m = len(y)\n",
    "        if m == 0:\n",
    "            return 0\n",
    "        counts = np.bincount(y)\n",
    "        probs = counts / m\n",
    "        return 1.0 - np.sum(probs**2)\n",
    "\n",
    "    def _best_split(self, X, y):\n",
    "        m, n = X.shape\n",
    "        if m < 2:\n",
    "            return None, None\n",
    "        best_gini = 1.0\n",
    "        best_idx, best_thr = None, None\n",
    "        current_gini = self._gini(y)\n",
    "        for idx in range(n):\n",
    "            thresholds = np.unique(X[:, idx])\n",
    "            for thr in thresholds:\n",
    "                left_mask = X[:, idx] <= thr\n",
    "                y_left, y_right = y[left_mask], y[~left_mask]\n",
    "                if len(y_left) == 0 or len(y_right) == 0:\n",
    "                    continue\n",
    "                g_left = self._gini(y_left)\n",
    "                g_right = self._gini(y_right)\n",
    "                g = (len(y_left) * g_left + len(y_right) * g_right) / m\n",
    "                if g < best_gini:\n",
    "                    best_gini = g\n",
    "                    best_idx = idx\n",
    "                    best_thr = thr\n",
    "        return best_idx, best_thr\n",
    "\n",
    "    def _build(self, X, y, depth=0):\n",
    "        num_samples_per_class = [np.sum(y == i) for i in range(np.max(y)+1)]\n",
    "        predicted_class = np.argmax(num_samples_per_class)\n",
    "        node = self.Node(gini=self._gini(y), n_samples=len(y), predicted_class=predicted_class)\n",
    "        if depth < self.max_depth and len(y) >= self.min_samples_split and node.gini > 0.0:\n",
    "            idx, thr = self._best_split(X, y)\n",
    "            if idx is not None:\n",
    "                left_mask = X[:, idx] <= thr\n",
    "                node.feature_index = idx\n",
    "                node.threshold = thr\n",
    "                node.left = self._build(X[left_mask], y[left_mask], depth+1)\n",
    "                node.right = self._build(X[~left_mask], y[~left_mask], depth+1)\n",
    "        return node\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.n_classes_ = len(np.unique(y))\n",
    "        self.n_features_ = X.shape[1]\n",
    "        self.root = self._build(X, y)\n",
    "        return self\n",
    "\n",
    "    def _predict_one(self, inputs, node):\n",
    "        while node.left:\n",
    "            if inputs[node.feature_index] <= node.threshold:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "        return node.predicted_class\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._predict_one(x, self.root) for x in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c0637aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Supervised: Random Forest (bagging of our trees)\n",
    "class RandomForestClassifierSimple:\n",
    "    \"\"\"\n",
    "    Simple Random Forest using bootstrap samples and a subset of features at each split.\n",
    "    We'll keep it minimal: bagging our DecisionTreeClassifierSimple but with feature subsampling\n",
    "    implemented by passing a subset of features to the tree's fit.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_estimators=10, max_depth=5, min_samples_split=2, max_features='sqrt', random_state=0):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_features = max_features\n",
    "        self.rng = np.random.RandomState(random_state)\n",
    "        self.trees = []\n",
    "        self.features_list = []\n",
    "\n",
    "    def _max_features_count(self, p):\n",
    "        if self.max_features == 'sqrt':\n",
    "            return max(1, int(np.sqrt(p)))\n",
    "        elif self.max_features == 'log2':\n",
    "            return max(1, int(np.log2(p)))\n",
    "        elif isinstance(self.max_features, int):\n",
    "            return self.max_features\n",
    "        else:\n",
    "            return p\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n, p = X.shape\n",
    "        self.trees = []\n",
    "        self.features_list = []\n",
    "        m = self._max_features_count(p)\n",
    "        for _ in range(self.n_estimators):\n",
    "            idxs = self.rng.choice(n, n, replace=True)\n",
    "            feat_idxs = self.rng.choice(p, m, replace=False)\n",
    "            tree = DecisionTreeClassifierSimple(max_depth=self.max_depth, min_samples_split=self.min_samples_split)\n",
    "            tree.fit(X[idxs][:, feat_idxs], y[idxs])\n",
    "            self.trees.append(tree)\n",
    "            self.features_list.append(feat_idxs)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        preds = []\n",
    "        for tree, feats in zip(self.trees, self.features_list):\n",
    "            preds.append(tree.predict(X[:, feats]))\n",
    "        preds = np.array(preds)\n",
    "        # majority vote along axis 0\n",
    "        maj = [Counter(col).most_common(1)[0][0] for col in preds.T]\n",
    "        return np.array(maj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8f69b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Supervised: Linear SVM (primal)\n",
    "\n",
    "class LinearSVM_SGD:\n",
    "    \"\"\"\n",
    "    Linear SVM via SGD on the primal hinge-loss objective with L2 regularization.\n",
    "    Minimizes: (1/2)||w||^2 + C * sum(max(0, 1 - y*(w^T x)))\n",
    "    where y in {-1, +1}.\n",
    "    \"\"\"\n",
    "    def __init__(self, lr=1e-3, n_iters=1000, C=1.0, fit_intercept=True):\n",
    "        self.lr = lr\n",
    "        self.n_iters = n_iters\n",
    "        self.C = C\n",
    "        self.fit_intercept = fit_intercept\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        y = np.where(y<=0, -1, 1)\n",
    "        X0 = np.hstack([np.ones((X.shape[0], 1)), X]) if self.fit_intercept else X\n",
    "        n, p = X0.shape\n",
    "        w = np.zeros(p)\n",
    "        for it in range(self.n_iters):\n",
    "            margins = y * (X0.dot(w))\n",
    "            # gradient from hinge loss\n",
    "            mask = margins < 1\n",
    "            grad = w - self.C * X0.T.dot(y * mask)\n",
    "            w -= self.lr * grad\n",
    "        if self.fit_intercept:\n",
    "            self.intercept_ = w[0]\n",
    "            self.coef_ = w[1:]\n",
    "        else:\n",
    "            self.intercept_ = 0.0\n",
    "            self.coef_ = w\n",
    "        return self\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        return X.dot(self.coef_) + self.intercept_\n",
    "\n",
    "    def predict(self, X):\n",
    "        return (self.decision_function(X) >= 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "972a5c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Supervised: Gaussian Naive Bayes\n",
    "\n",
    "class GaussianNB:\n",
    "    \"\"\"\n",
    "    Gaussian Naive Bayes from scratch: model each feature per-class as Gaussian with estimated mean & var.\n",
    "    \"\"\"\n",
    "    def fit(self, X, y):\n",
    "        self.classes_ = np.unique(y)\n",
    "        self.means_ = {}\n",
    "        self.vars_ = {}\n",
    "        self.priors_ = {}\n",
    "        for c in self.classes_:\n",
    "            Xc = X[y==c]\n",
    "            self.means_[c] = Xc.mean(axis=0)\n",
    "            self.vars_[c] = Xc.var(axis=0) + 1e-9\n",
    "            self.priors_[c] = Xc.shape[0] / X.shape[0]\n",
    "        return self\n",
    "\n",
    "    def _log_gaussian(self, x, mean, var):\n",
    "        return -0.5 * np.sum(np.log(2*np.pi*var)) -0.5 * np.sum(((x-mean)**2) / var)\n",
    "\n",
    "    def predict(self, X):\n",
    "        preds = []\n",
    "        for x in X:\n",
    "            best_c, best_logp = None, -np.inf\n",
    "            for c in self.classes_:\n",
    "                logp = np.log(self.priors_[c]) + self._log_gaussian(x, self.means_[c], self.vars_[c])\n",
    "                if logp > best_logp:\n",
    "                    best_logp = logp\n",
    "                    best_c = c\n",
    "            preds.append(best_c)\n",
    "        return np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28f18487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unsupervised: k-Means\n",
    "\n",
    "class KMeans:\n",
    "    \"\"\"\n",
    "    Basic k-means clustering using Lloyd's algorithm.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_clusters=3, n_iters=100, random_state=0):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.n_iters = n_iters\n",
    "        self.rng = np.random.RandomState(random_state)\n",
    "\n",
    "    def fit(self, X):\n",
    "        n, d = X.shape\n",
    "        centers = X[self.rng.choice(n, self.n_clusters, replace=False)]\n",
    "        for _ in range(self.n_iters):\n",
    "            dists = np.linalg.norm(X[:, None, :] - centers[None, :, :], axis=2)\n",
    "            labels = dists.argmin(axis=1)\n",
    "            new_centers = np.array([X[labels==k].mean(axis=0) if np.any(labels==k) else centers[k] for k in range(self.n_clusters)])\n",
    "            if np.allclose(new_centers, centers):\n",
    "                break\n",
    "            centers = new_centers\n",
    "        self.cluster_centers_ = centers\n",
    "        self.labels_ = labels\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14be8c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unsupervised: Principal Component Analysis (PCA)\n",
    "\n",
    "class PCA:\n",
    "    \"\"\"\n",
    "    Principal Component Analysis via SVD.\n",
    "    Returns components and explained variance.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_components=None):\n",
    "        self.n_components = n_components\n",
    "\n",
    "    def fit(self, X):\n",
    "        Xc = X - X.mean(axis=0)\n",
    "        U, S, Vt = np.linalg.svd(Xc, full_matrices=False)\n",
    "        components = Vt\n",
    "        explained_variance = (S**2) / (X.shape[0]-1)\n",
    "        if self.n_components is not None:\n",
    "            components = components[:self.n_components]\n",
    "            explained_variance = explained_variance[:self.n_components]\n",
    "        self.components_ = components\n",
    "        self.explained_variance_ = explained_variance\n",
    "        self.mean_ = X.mean(axis=0)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        Xc = X - self.mean_\n",
    "        return Xc.dot(self.components_.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8177335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unsupervised: Gaussian Mixture Model (EM)\n",
    "\n",
    "class GaussianMixtureSimple:\n",
    "    \"\"\"\n",
    "    Simplified Gaussian Mixture Model with full covariance and EM - The Expectation-Maximization (EM) algorithm is a common method\n",
    "    for training GMMs, iteratively refining the model parameters (means, covariances, and mixing coefficients of the Gaussians) \n",
    "    to maximize the likelihood of the observed data. \n",
    "    \"\"\"\n",
    "    def __init__(self, n_components=2, n_iters=100, tol=1e-4, random_state=0):\n",
    "        self.n_components = n_components\n",
    "        self.n_iters = n_iters\n",
    "        self.tol = tol\n",
    "        self.rng = np.random.RandomState(random_state)\n",
    "\n",
    "    def _multivariate_gaussian(self, X, mean, cov):\n",
    "        d = X.shape[1]\n",
    "        cov_inv = np.linalg.inv(cov)\n",
    "        denom = np.sqrt(((2*np.pi)**d) * np.linalg.det(cov))\n",
    "        diffs = X - mean\n",
    "        exponents = np.einsum('ij,jk,ik->i', diffs, cov_inv, diffs)\n",
    "        return np.exp(-0.5*exponents) / (denom + 1e-12)\n",
    "\n",
    "    def fit(self, X):\n",
    "        n, d = X.shape\n",
    "        # initialize\n",
    "        means = X[self.rng.choice(n, self.n_components, replace=False)]\n",
    "        covs = np.array([np.cov(X, rowvar=False) + 1e-6*np.eye(d) for _ in range(self.n_components)])\n",
    "        pis = np.ones(self.n_components) / self.n_components\n",
    "        prev_ll = None\n",
    "        for it in range(self.n_iters):\n",
    "            # E-step\n",
    "            resp = np.zeros((n, self.n_components))\n",
    "            for k in range(self.n_components):\n",
    "                resp[:, k] = pis[k] * self._multivariate_gaussian(X, means[k], covs[k])\n",
    "            row_sums = resp.sum(axis=1)[:, None]\n",
    "            resp = resp / (row_sums + 1e-12)\n",
    "            Nk = resp.sum(axis=0)\n",
    "            # M-step\n",
    "            for k in range(self.n_components):\n",
    "                means[k] = (resp[:, k][:, None] * X).sum(axis=0) / Nk[k]\n",
    "                diff = X - means[k]\n",
    "                covs[k] = (resp[:, k][:, None, None] * np.einsum('...i,...j->...ij', diff, diff)).sum(axis=0) / Nk[k] + 1e-6*np.eye(d)\n",
    "                pis[k] = Nk[k] / n\n",
    "            # log-likelihood\n",
    "            ll = np.sum(np.log((resp * pis).sum(axis=1) + 1e-12))\n",
    "            if prev_ll is not None and abs(ll - prev_ll) < self.tol:\n",
    "                break\n",
    "            prev_ll = ll\n",
    "        self.means_ = means\n",
    "        self.covariances_ = covs\n",
    "        self.weights_ = pis\n",
    "        self.resp_ = resp\n",
    "        self.labels_ = resp.argmax(axis=1)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b528e58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Linear Regression (closed form) demo\n",
      "true_w: [-1.30652685  1.65813068 -0.11816405]\n",
      "estimated coef: [-1.25959699  1.61345348 -0.14386056]\n",
      "\n",
      "--- Logistic Regression demo\n",
      "logreg accuracy: 1.0\n",
      "\n",
      "--- k-NN demo\n",
      "knn accuracy: 1.0\n",
      "\n",
      "--- Decision Tree demo\n",
      "tree accuracy: 1.0\n",
      "\n",
      "--- Random Forest demo\n",
      "rf accuracy: 1.0\n",
      "\n",
      "--- Linear SVM demo\n",
      "svm accuracy: 1.0\n",
      "\n",
      "--- GaussianNB demo\n",
      "gnb accuracy: 1.0\n",
      "\n",
      "--- k-Means demo\n",
      "kmeans cluster size: [75 75]\n",
      "\n",
      "--- PCA demo\n",
      "pca components shape: (2, 2)\n",
      "\n",
      "--- GMM demo\n",
      "gmm weights: [0.65817303 0.34182697]\n",
      "\n",
      "--- End of demos. For Keras/TensorFlow examples see the Keras_usage_snippet in the file.\n"
     ]
    }
   ],
   "source": [
    "# Demonstration / quick tests\n",
    "\n",
    "def _demo():\n",
    "    print('\\n--- Linear Regression (closed form) demo')\n",
    "    X, y, true_w = make_regression_data(n=100, d=3, noise=0.5)\n",
    "    lr = LinearRegressionClosedForm(lam=1e-3)\n",
    "    lr.fit(X, y)\n",
    "    print('true_w:', true_w)\n",
    "    print('estimated coef:', lr.coef_)\n",
    "\n",
    "    print('\\n--- Logistic Regression demo')\n",
    "    Xc, yc = make_classification_data(n=200, d=2)\n",
    "    logreg = LogisticRegressionGD(lr=0.1, n_iters=2000)\n",
    "    logreg.fit(Xc, yc)\n",
    "    print('logreg accuracy:', (logreg.predict(Xc) == yc).mean())\n",
    "\n",
    "    print('\\n--- k-NN demo')\n",
    "    knn = KNNClassifier(k=5)\n",
    "    knn.fit(Xc, yc)\n",
    "    print('knn accuracy:', (knn.predict(Xc) == yc).mean())\n",
    "\n",
    "    print('\\n--- Decision Tree demo')\n",
    "    tree = DecisionTreeClassifierSimple(max_depth=5)\n",
    "    tree.fit(Xc, yc)\n",
    "    print('tree accuracy:', (tree.predict(Xc) == yc).mean())\n",
    "\n",
    "    print('\\n--- Random Forest demo')\n",
    "    rf = RandomForestClassifierSimple(n_estimators=5, max_depth=5)\n",
    "    rf.fit(Xc, yc)\n",
    "    print('rf accuracy:', (rf.predict(Xc) == yc).mean())\n",
    "\n",
    "    print('\\n--- Linear SVM demo')\n",
    "    svm = LinearSVM_SGD(lr=1e-3, n_iters=2000)\n",
    "    svm.fit(Xc, yc)\n",
    "    print('svm accuracy:', (svm.predict(Xc) == yc).mean())\n",
    "\n",
    "    print('\\n--- GaussianNB demo')\n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(Xc, yc)\n",
    "    print('gnb accuracy:', (gnb.predict(Xc) == yc).mean())\n",
    "\n",
    "    print('\\n--- k-Means demo')\n",
    "    Xk, yk = make_classification_data(n=150, d=2)\n",
    "    km = KMeans(n_clusters=2)\n",
    "    km.fit(Xk)\n",
    "    print('kmeans cluster size:', np.bincount(km.labels_))\n",
    "\n",
    "    print('\\n--- PCA demo')\n",
    "    pca = PCA(n_components=2)\n",
    "    pca.fit(Xk)\n",
    "    print('pca components shape:', pca.components_.shape)\n",
    "\n",
    "    print('\\n--- GMM demo')\n",
    "    gmm = GaussianMixtureSimple(n_components=2, n_iters=50)\n",
    "    gmm.fit(Xk)\n",
    "    print('gmm weights:', gmm.weights_)\n",
    "\n",
    "    print('\\n--- End of demos. For Keras/TensorFlow examples see the Keras_usage_snippet in the file.')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    _demo()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
