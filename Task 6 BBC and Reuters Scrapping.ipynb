{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95678b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pandas numpy scikit-learn joblib requests beautifulsoup4 matplotlib\n",
    "# optional (richer article text): newspaper3k\n",
    "!pip install -q newspaper3k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6554ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup done\n"
     ]
    }
   ],
   "source": [
    "import os, re, json, joblib, math\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Make folders\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "os.makedirs(\"unlabeled\", exist_ok=True)\n",
    "\n",
    "print(\"Setup done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88e561ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw shape: (209527, 6)\n",
      "Filtered to 5 classes: (68662, 2)\n",
      "super_category\n",
      "Politics      47965\n",
      "Business       7748\n",
      "Sports         5077\n",
      "Technology     4310\n",
      "Crime          3562\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load Kaggle dataset\n",
    "# Path to the JSON you have already: data/News_Category_Dataset_v3.json\n",
    "DATA_PATH = \"data/News_Category_Dataset_v3.json\"\n",
    "assert os.path.exists(DATA_PATH), f\"Dataset not found: {DATA_PATH}\"\n",
    "\n",
    "df_raw = pd.read_json(DATA_PATH, lines=True)\n",
    "print(\"raw shape:\", df_raw.shape)\n",
    "df_raw.head(3)\n",
    "\n",
    "# Compose text\n",
    "df_raw[\"text\"] = (df_raw[\"headline\"].fillna(\"\") + \". \" + df_raw[\"short_description\"].fillna(\"\")).str.strip()\n",
    "\n",
    "# Mapping: expand as you like\n",
    "mapping = {\n",
    "    # Politics-like\n",
    "    \"POLITICS\": \"Politics\", \"WORLD NEWS\": \"Politics\", \"U.S. NEWS\": \"Politics\",\n",
    "    \"THE WORLDPOST\": \"Politics\", \"WORLDPOST\": \"Politics\", \"ENVIRONMENT\": \"Politics\",\n",
    "    # Sports\n",
    "    \"SPORTS\": \"Sports\",\n",
    "    # Business\n",
    "    \"BUSINESS\": \"Business\", \"MONEY\": \"Business\",\n",
    "    # Technology / Science -> Technology\n",
    "    \"TECH\": \"Technology\", \"TECHNOLOGY\": \"Technology\", \"SCIENCE\": \"Technology\",\n",
    "    # Crime\n",
    "    \"CRIME\": \"Crime\",\n",
    "    # (Add more HuffPost categories that you want to fold into these five)\n",
    "}\n",
    "\n",
    "# Uppercase categories then map\n",
    "df_raw[\"super_category\"] = df_raw[\"category\"].str.upper().map(mapping)\n",
    "\n",
    "# Keep only rows mapped to our 5 classes and non-empty text\n",
    "df = df_raw.dropna(subset=[\"super_category\", \"text\"]).copy()\n",
    "df = df[[\"text\", \"super_category\"]].reset_index(drop=True)\n",
    "print(\"Filtered to 5 classes:\", df.shape)\n",
    "print(df[\"super_category\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b78d43c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/test sizes: 54929 13733\n",
      "TF-IDF shapes: (54929, 30000) (13733, 30000)\n"
     ]
    }
   ],
   "source": [
    "# Clean text (simple, fast), split and vectorize for supervised model\n",
    "def clean_text_simple(s):\n",
    "    if not isinstance(s, str):\n",
    "        s = str(s)\n",
    "    s = s.lower()\n",
    "    # keep letters, numbers and spaces\n",
    "    s = re.sub(r\"[^a-z0-9\\s]\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "df[\"text_clean\"] = df[\"text\"].apply(clean_text_simple)\n",
    "\n",
    "# Train/test split (stratify by super_category)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[\"text_clean\"], df[\"super_category\"], test_size=0.20, random_state=42, stratify=df[\"super_category\"]\n",
    ")\n",
    "\n",
    "print(\"train/test sizes:\", len(X_train), len(X_test))\n",
    "\n",
    "# Supervised TF-IDF (this is the vectorizer the classifier expects)\n",
    "supervised_tfidf = TfidfVectorizer(stop_words=\"english\", max_features=30000, ngram_range=(1,2), min_df=3)\n",
    "X_train_tfidf = supervised_tfidf.fit_transform(X_train)\n",
    "X_test_tfidf  = supervised_tfidf.transform(X_test)\n",
    "\n",
    "print(\"TF-IDF shapes:\", X_train_tfidf.shape, X_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dbe4f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8709\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Business       0.76      0.69      0.72      1550\n",
      "       Crime       0.74      0.62      0.67       712\n",
      "    Politics       0.90      0.95      0.92      9593\n",
      "      Sports       0.86      0.83      0.84      1016\n",
      "  Technology       0.77      0.60      0.67       862\n",
      "\n",
      "    accuracy                           0.87     13733\n",
      "   macro avg       0.81      0.74      0.77     13733\n",
      "weighted avg       0.87      0.87      0.87     13733\n",
      "\n",
      "Saved supervised model + vectorizer.\n"
     ]
    }
   ],
   "source": [
    "# Train classifier (LinearSVC) and evaluate — save model & vectorizer\n",
    "clf = LinearSVC(random_state=42, max_iter=5000)   # robust for text\n",
    "clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test_tfidf)\n",
    "print(\"Accuracy:\", round(accuracy_score(y_test, y_pred), 4))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Save supervised artifacts\n",
    "joblib.dump(clf, \"models/supervised_linear_svc.joblib\")\n",
    "joblib.dump(supervised_tfidf, \"models/supervised_tfidf.joblib\")\n",
    "print(\"Saved supervised model + vectorizer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17b131aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping BBC...\n",
      "Scraping Reuters...\n",
      "Unlabeled articles count: 20\n"
     ]
    }
   ],
   "source": [
    "# Scrape BBC and Reuters for unlabeled recent news (full article text when possible)\n",
    "# NOTE: scraping structure may change on websites. This is a robust, conservative approach:\n",
    "#  - collect candidate article links from the index pages\n",
    "#  - fetch each article and join <p> texts\n",
    "\n",
    "def fetch_article_paragraphs(url):\n",
    "    try:\n",
    "        r = requests.get(url, timeout=10, headers={\"User-Agent\":\"Mozilla/5.0\"})\n",
    "        if r.status_code != 200:\n",
    "            return None\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "        # collect <p> text\n",
    "        paras = [p.get_text().strip() for p in soup.find_all(\"p\")]\n",
    "        text = \" \".join([p for p in paras if p])\n",
    "        # keep if long enough\n",
    "        return text if len(text.split()) > 40 else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def scrape_bbc(n=30):\n",
    "    base = \"https://www.bbc.com\"\n",
    "    idx = \"https://www.bbc.com/news\"\n",
    "    r = requests.get(idx, headers={\"User-Agent\":\"Mozilla/5.0\"}, timeout=10)\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    links = set()\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        href = a[\"href\"]\n",
    "        if href.startswith(\"/news\") or href.startswith(\"/sport\"):\n",
    "            # build absolute URL\n",
    "            url = href if href.startswith(\"http\") else base + href\n",
    "            links.add(url)\n",
    "        if len(links) >= n*2:\n",
    "            break\n",
    "    texts = []\n",
    "    for url in list(links)[: n*3]:\n",
    "        t = fetch_article_paragraphs(url)\n",
    "        if t:\n",
    "            texts.append(t)\n",
    "        if len(texts) >= n:\n",
    "            break\n",
    "    return texts\n",
    "\n",
    "def scrape_reuters(n=30):\n",
    "    base = \"https://www.reuters.com\"\n",
    "    idx = \"https://www.reuters.com/world/\"\n",
    "    r = requests.get(idx, headers={\"User-Agent\":\"Mozilla/5.0\"}, timeout=10)\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    links = set()\n",
    "    # Reuters uses <a href=\"/world/...\"> in many places\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        href = a[\"href\"]\n",
    "        if href.startswith(\"/world\") or href.startswith(\"/business\") or href.startswith(\"/politics\") or href.startswith(\"/technology\"):\n",
    "            url = href if href.startswith(\"http\") else base + href\n",
    "            links.add(url)\n",
    "        if len(links) >= n*2:\n",
    "            break\n",
    "    texts = []\n",
    "    for url in list(links)[: n*3]:\n",
    "        t = fetch_article_paragraphs(url)\n",
    "        if t:\n",
    "            texts.append(t)\n",
    "        if len(texts) >= n:\n",
    "            break\n",
    "    return texts\n",
    "\n",
    "# Try scraping — falls back to reading ./unlabeled/*.txt if no internet or site blocks requests\n",
    "unlabeled_texts = []\n",
    "try:\n",
    "    print(\"Scraping BBC...\")\n",
    "    unlabeled_texts += scrape_bbc(n=20)\n",
    "    print(\"Scraping Reuters...\")\n",
    "    unlabeled_texts += scrape_reuters(n=20)\n",
    "except Exception as e:\n",
    "    print(\"Scrape exception:\", e)\n",
    "\n",
    "# If scraping yields nothing, load local .txt files\n",
    "if not unlabeled_texts:\n",
    "    for p in Path(\"unlabeled\").glob(\"*.txt\"):\n",
    "        t = p.read_text(errors=\"ignore\")\n",
    "        if len(t.split()) > 30:\n",
    "            unlabeled_texts.append(t)\n",
    "\n",
    "# Fallback (very small) demo if still empty (so pipeline runs)\n",
    "if not unlabeled_texts:\n",
    "    unlabeled_texts = [\n",
    "        \"Government launches election campaign focusing on healthcare and education reform.\",\n",
    "        \"Local football team clinches championship after a dramatic comeback in extra time.\",\n",
    "        \"Major tech firms invest heavily in artificial intelligence and cloud infrastructure.\",\n",
    "        \"Police investigate a corruption scandal around procurement contracts.\",\n",
    "        \"Global markets rise as central banks signal potential rate pauses.\"\n",
    "    ]\n",
    "\n",
    "print(\"Unlabeled articles count:\", len(unlabeled_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38b51fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsupervised TF-IDF shape: (20, 5000)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['models/unsup_tfidf.joblib']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vectorize unlabeled corpus (separate unsupervised vectorizer\n",
    "# Use a smaller TF-IDF for unsupervised clustering (keeps resources modest)\n",
    "unsup_tfidf = TfidfVectorizer(stop_words=\"english\", max_features=5000, ngram_range=(1,2))\n",
    "X_unsup = unsup_tfidf.fit_transform([clean_text_simple(t) for t in unlabeled_texts])\n",
    "\n",
    "print(\"Unsupervised TF-IDF shape:\", X_unsup.shape)\n",
    "\n",
    "# Save unsupervised vectorizer\n",
    "joblib.dump(unsup_tfidf, \"models/unsup_tfidf.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d61d9322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen K: 5 silhouette: 0.02922705910687092\n",
      "\n",
      "Cluster 0 top terms: trump, said, plaza, like, afghans, president, ceasefire, film, peace, ukraine\n",
      "\n",
      "Cluster 1 top terms: minister, hostages, gaza, prime minister, prime, israeli, israel, hamas, senedd, pengelly\n",
      "\n",
      "Cluster 2 top terms: australia, korea, north korea, north, floods, wen, flash, missing, china, people\n",
      "\n",
      "Cluster 3 top terms: homes, says, councils, housing, long term, farage, controlled councils, says reform, close hotels, nigel farage\n",
      "\n",
      "Cluster 4 top terms: m23, phillips, zealand, new zealand, children, mr phillips, group, said, conflict, mr\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['models/kmeans_topics.joblib']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# KMeans clustering\n",
    "# choose a reasonable set of k to try\n",
    "candidate_k = [3,4,5,6,7,8] if X_unsup.shape[0] >= 30 else [2,3,4,5]\n",
    "best_k, best_score, best_km = None, -1, None\n",
    "for k in candidate_k:\n",
    "    if k >= X_unsup.shape[0]:\n",
    "        continue\n",
    "    km = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels = km.fit_predict(X_unsup)\n",
    "    if len(set(labels)) > 1:\n",
    "        try:\n",
    "            s = silhouette_score(X_unsup, labels)\n",
    "        except Exception:\n",
    "            s = -1\n",
    "        if s > best_score:\n",
    "            best_k, best_score, best_km = k, s, km\n",
    "\n",
    "if best_k is None:\n",
    "    best_k = min(5, max(2, X_unsup.shape[0]//5))\n",
    "    best_km = KMeans(n_clusters=best_k, random_state=42, n_init=10).fit(X_unsup)\n",
    "\n",
    "kmeans = best_km\n",
    "print(\"Chosen K:\", best_k, \"silhouette:\", best_score)\n",
    "\n",
    "# Extract top terms per cluster\n",
    "def top_terms_kmeans(km_model, vectorizer, topn=10):\n",
    "    centers = km_model.cluster_centers_\n",
    "    terms = np.array(vectorizer.get_feature_names_out())\n",
    "    topics = {}\n",
    "    for idx, center in enumerate(centers):\n",
    "        top_idx = np.argsort(center)[::-1][:topn]\n",
    "        topics[idx] = terms[top_idx].tolist()\n",
    "    return topics\n",
    "\n",
    "kmeans_topics = top_terms_kmeans(kmeans, unsup_tfidf, topn=10)\n",
    "for cid, terms in kmeans_topics.items():\n",
    "    print(f\"\\nCluster {cid} top terms: {', '.join(terms)}\")\n",
    "\n",
    "# Save kmeans + cluster terms\n",
    "joblib.dump(kmeans, \"models/kmeans.joblib\")\n",
    "joblib.dump(kmeans_topics, \"models/kmeans_topics.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "573ed361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LDA Topic 0: trump, said, north, president, ceasefire, say, peace, bbc, taliban, country\n",
      "\n",
      "LDA Topic 1: minister, bbc, palestinian, israeli, day, prime, gaza, says, israel, 60\n",
      "\n",
      "LDA Topic 2: says, bbc, external, new, people, uk, australia, children, police, rights\n",
      "\n",
      "LDA Topic 3: homes, said, councils, bbc, m23, says, housing, children, government, people\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['models/lda_topics.joblib']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA topic modeling on unlabeled texts (CountVectorizer + LDA)\n",
    "count_vec = CountVectorizer(stop_words=\"english\", max_features=5000, min_df=2)\n",
    "X_counts = count_vec.fit_transform([clean_text_simple(t) for t in unlabeled_texts])\n",
    "\n",
    "n_topics = min(8, max(2, len(unlabeled_texts)//5))  # heuristic\n",
    "lda = LatentDirichletAllocation(n_components=n_topics, random_state=42, learning_method=\"batch\")\n",
    "lda.fit(X_counts)\n",
    "\n",
    "def lda_top_words(model, feature_names, n_top=8):\n",
    "    out = {}\n",
    "    for ti, comp in enumerate(model.components_):\n",
    "        idxs = np.argsort(comp)[::-1][:n_top]\n",
    "        out[ti] = [feature_names[i] for i in idxs]\n",
    "    return out\n",
    "\n",
    "lda_topics = lda_top_words(lda, count_vec.get_feature_names_out(), n_top=10)\n",
    "for tid, words in lda_topics.items():\n",
    "    print(f\"\\nLDA Topic {tid}: {', '.join(words)}\")\n",
    "\n",
    "# Save LDA artifacts\n",
    "joblib.dump(lda, \"models/lda_model.joblib\")\n",
    "joblib.dump(count_vec, \"models/lda_countvec.joblib\")\n",
    "joblib.dump(lda_topics, \"models/lda_topics.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15bd8aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unified prediction function — supervised + unsupervised labels (human readable)\n",
    "# Load artifacts (if needed)\n",
    "supervised_tfidf = joblib.load(\"models/supervised_tfidf.joblib\")\n",
    "clf = joblib.load(\"models/supervised_linear_svc.joblib\")\n",
    "unsup_tfidf = joblib.load(\"models/unsup_tfidf.joblib\")\n",
    "kmeans = joblib.load(\"models/kmeans.joblib\")\n",
    "kmeans_topics = joblib.load(\"models/kmeans_topics.joblib\")\n",
    "lda = joblib.load(\"models/lda_model.joblib\")\n",
    "count_vec = joblib.load(\"models/lda_countvec.joblib\")\n",
    "lda_topics = joblib.load(\"models/lda_topics.joblib\")\n",
    "\n",
    "def predict_article(text):\n",
    "    # Clean text first (same cleaning as used in training)\n",
    "    s = clean_text_simple(text)\n",
    "    # Supervised category (one of the five)\n",
    "    v_sup = supervised_tfidf.transform([s])\n",
    "    cat = clf.predict(v_sup)[0]\n",
    "\n",
    "    # Unsupervised cluster (KMeans)\n",
    "    v_unsup = unsup_tfidf.transform([s])\n",
    "    cl_id = int(kmeans.predict(v_unsup)[0])\n",
    "    cl_terms = kmeans_topics.get(cl_id, [])\n",
    "    cl_label = \", \".join(cl_terms[:6]) if cl_terms else f\"Cluster {cl_id}\"\n",
    "\n",
    "    # LDA topic (top topic)\n",
    "    c = count_vec.transform([s])\n",
    "    topic_dist = lda.transform(c)[0]\n",
    "    lda_top = int(np.argmax(topic_dist))\n",
    "    lda_label = \", \".join(lda_topics.get(lda_top, [])[:6]) if lda_topics.get(lda_top) else f\"Topic {lda_top}\"\n",
    "\n",
    "    return {\n",
    "        \"supervised_category\": cat,\n",
    "        \"kmeans_cluster_id\": cl_id,\n",
    "        \"kmeans_topic_label\": cl_label,\n",
    "        \"lda_top_topic_id\": lda_top,\n",
    "        \"lda_topic_label\": lda_label\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c63683cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAMPLE TEXT:\n",
      " In 1980, when Corina Poore, 36 years old and pregnant, first opened the door to a derelict house in New Cross Gate, south-east London, the estate agent refused to step in with her. Inside were dead cats, dog excrement and filthy mattresses. Pigeons flew in through holes in the roof and there was no indoor toilet. The intense rotting smell was overwhelming. Still, Corina decided this was her dream home. It was spacious, the £24,000 price was affordable and she was sure that everything was fixable. After taking out a mortgage, she received a grant of £3,500 from Lewisham council, her local authority, which paid for fixing the ceiling. \"At that point, £3,500 was quite a healthy amount, which I desperately needed,\" recalls Corina. Some 45 years on, her Victorian four-storey house is worth roughly £1m - something Corina, a semi-retired film and TV critic, could never have afforded otherwise. However, times have changed. Lewisham Council has continued to offer grants to the owners of empty h \n",
      "\n",
      "PREDICTION: {'supervised_category': 'Business', 'kmeans_cluster_id': 3, 'kmeans_topic_label': 'homes, says, councils, housing, long term, farage', 'lda_top_topic_id': 3, 'lda_topic_label': 'homes, said, councils, bbc, m23, says'}\n",
      "USER PREDICTION: {'supervised_category': 'Politics', 'kmeans_cluster_id': 2, 'kmeans_topic_label': 'australia, korea, north korea, north, floods, wen', 'lda_top_topic_id': 2, 'lda_topic_label': 'says, bbc, external, new, people, uk'}\n"
     ]
    }
   ],
   "source": [
    "# Try it — examples and interactive input\n",
    "# Try a scraped sample\n",
    "sample = unlabeled_texts[0]\n",
    "print(\"SAMPLE TEXT:\\n\", sample[:1000], \"\\n\")\n",
    "print(\"PREDICTION:\", predict_article(sample))\n",
    "\n",
    "# Try arbitrary user text:\n",
    "user_text = \"Authorities opened a corruption probe into government contracts ahead of the election campaign.\"\n",
    "print(\"USER PREDICTION:\", predict_article(user_text))\n",
    "\n",
    "# Optional: interactive loop (comment/uncomment as needed)\n",
    "# while True:\n",
    "#     u = input(\"Paste article (or 'exit'): \")\n",
    "#     if u.strip().lower() == \"exit\":\n",
    "#         break\n",
    "#     print(predict_article(u))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "acb17530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'trump / said / plaza / like / afghans',\n",
       " 1: 'minister / hostages / gaza / prime minister / prime',\n",
       " 2: 'australia / korea / north korea / north / floods',\n",
       " 3: 'homes / says / councils / housing / long term',\n",
       " 4: 'm23 / phillips / zealand / new zealand / children'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map cluster IDs → human labels (semi-automatic)\n",
    "# Create a readable mapping for each KMeans cluster based on top terms\n",
    "cluster_label_map = {}\n",
    "for cid, terms in kmeans_topics.items():\n",
    "    # naive label: join top 3 keywords\n",
    "    cluster_label_map[cid] = \" / \".join(terms[:5])\n",
    "cluster_label_map\n",
    "# You can manually rename cluster_label_map[cid] = \"Ukraine War\" etc. after inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf35b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steps above code is performing according to given Task:\n",
    "1.\tLoad & map Kaggle dataset → into 5 categories (Politics, Sports, Business, Technology, Crime).\n",
    "2.\tPreprocess & clean text → train/test split, TF-IDF vectorization.\n",
    "3.\tTrain classifier (LinearSVC) → evaluate, save model + vectorizer.\n",
    "4.\tScrape BBC & Reuters (or fallback to local .txt unlabeled articles).\n",
    "5.\tUnsupervised text processing → separate vectorizer for clustering (no overwrite).\n",
    "6.\tKMeans clustering → auto-select k (via silhouette score), extract top terms, save topics.\n",
    "7.\tLDA topic modeling → discover latent topics with keywords.\n",
    "8.\tUnified prediction function (predict_article) →\n",
    "9.\tGives supervised category (one of the 5 classes).\n",
    "10.\tReturns unsupervised cluster/topic labels (KMeans + LDA).\n",
    "11.\tTesting & interactive use → sample prediction on scraped or user input.\n",
    "12.\tOptional cluster label mapping → convert numeric clusters into human-readable names."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
